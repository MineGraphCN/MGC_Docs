# 光影 基本概念

<primary-label ref="basic"/>

<secondary-label ref="latest"/>
<secondary-label ref="shader"/>

<show-structure depth="2"/>

## 何谓光影？ {id="here_sASimpleQuestion_whatIsShader"}

对于一个完整的现代图形应用程序（基于如 OpenGL、Vulkan、DirectX 等图形库）来说，着色器是它渲染场景的手段。  
我们知道图形应用程序的目的是读取模型文件或硬编码几何体，并在屏幕上绘制。着色器就描述了我们传入的几何体在屏幕上的**何种位置**以**何种方式**绘制。

如果将资源包比作食材，那么着色器就是调料，上好的食材固然重要，但是调料也是激发出食材风味不可或缺的部分。

> 光影是着色器的集合。

## 渲染模组／引擎如何帮助光影和游戏交流

所谓渲染模组和引擎（下简称渲染模组）也是一种**加载器**，和模组加载器充当游戏和模组的桥梁类似，它们充当了游戏和光影的桥梁。由于原版所提供的信息极其有限（许多效果必须的变量没有直接提供，[缓冲区](terms.md#缓冲区){summary=""} 也不足），如果想仅利用原版资源包的着色器来编写光影，无异于自讨苦吃。

渲染模组利用模组加载器提供的 [接口](terms.md#应用程序接口){summary=""} 或直接对 Minecraft 源代码进行逆向工程并注入，接管了 Minecraft 的原版渲染管线，并提供了大量信息和更多缓冲区，为光影开发提供便利。

> 一些渲染模组还修改了原本所使用的图形库，将 OpenGL 替换为了 Vulkan，如 **Vulkanite**。

## 场景在着色器中发生了什么？ {id="whatWasYourMissionInShader"}

### 着色器类型

光影通常以多个着色器组成，着色器接收渲染模组提供的各种变量，以及先前乃至上一帧计算好的存入缓冲区的信息，按照程序进行计算后输出到指定的缓冲区。  
不同渲染模组的工作原理不尽相同，所对应的光影包规格也有所区别。

一次完整的着色渲染流程又可以细分为多个阶段，也就是我们常说的顶点着色器、像素着色器等，这里按照通常管线的顺序，简单列举一下每种着色器计算的对象：

<deflist>

<def title="顶点着色器" id="vs">

**Vertex Shader**，它的主要职责是变换坐标，包括顶点坐标、纹理坐标等，也可以处理顶点的颜色，计算对象为每个顶点（逐顶点操作）。
</def>
<def title="几何着色器" id="gs">

**Geometry Shader**，这个阶段是**可选的**，它的主要职责是根据图元生成新顶点，可以通过特定的索引值确定需要处理的顶点。
> 图元通常为点（一个顶点）、线条（两个顶点或三个顶点 <sup>线框渲染多边形</sup> ）或多边形（三个以上的顶点）。
</def>
<def title="片段着色器" id="fs">

**Fragment Shader**，它的主要职责是处理像素的颜色，也是大多数效果程序所处的位置，计算对象是图元覆盖的每个像素。
> 根据它所计算的对象，我们也将其称为**像素着色器**（Pixel Shader）。
</def>

</deflist>

除了上述在 GL 管线中有固定顺序的着色器外，还有一个名为**计算着色器**的阶段，它可以在任意时间开始运行，且不与其他阶段挂钩。

<deflist>
<def title="计算着色器" id="cs">

**Compute Shader**，这个阶段是**可选的**，它负责进行抽象计算。可以任意存取缓冲区，但是不能传入自定义变量，也没有默认输出（不与其他阶段挂钩所致）。

在 OptiFine 和 Iris 管线中，计算着色器在对应 ID 的像素着色器**之前**运行。
> 计算着色器使 GPU 可以像 CPU 一样做通用计算（主要是大规模并行计算，GPU 并不擅长处理分支），使开发者可以更随性地写东西，例如高性能后处理模糊。
</def>
</deflist>

当仅考虑顶点着色器和像素着色器时，在 [上文](#here_sASimpleQuestion_whatIsShader "何谓光影？") 中我们所提到的所谓“*何种位置*”大多数时候就在顶点着色器中进行处理，而以“*何种方式*”则是顶点着色器和像素着色器的共同作用。

> 除了上述着色器以外，OpenGL 还在 4.0 引入了细分着色器（Tessellation Shader），其在顶点着色器之后开始运行，它的主要职责是将图元细分并提供额外的细节。由于其在 OptiFine 和 Iris 管线中不可用，这里不再展开叙述。

> 2018 年英伟达提出了 [网格着色器（Mesh Shader）](https://developer.nvidia.com/zh-cn/blog/introduction-turing-mesh-shaders/)，这让几何处理管线不再拘泥于传统的管线顺序，拥有更强的可编程性和性能，使得所有几何处理工作均可在 GPU 端完成，无需和 CPU 进行高延迟通信。它的本质是计算着色器。

### 光栅化

光栅化通过将场景映射到二维平面上并分划为一个个的小格子，且仅保留每个格子中心点的信息来将场景中的几何信息离散，然后一一对应到输出对象的最小单元（通常是缓冲区或屏幕的像素）上，也叫**栅格化**（与 Photoshop 等图像处理软件中的同名功能一致）。

场景离散之后每个像素上都有确定的几何信息（法线、位置等）和纹理信息，然后根据这些几何信息利用数学方法来处理每个像素内容，因此这种方法也会造成一定的**几何走样**（也就是我们通常所说的**锯齿**）。

“光栅化”这个说法通常是用作与光线追踪的对立：前者在栅格化之后再进行数学上的各种光照拟合和其他后期处理；而后者通过模拟光线路径计算光照之后直到需要输出图像或者进行其他后期处理时才栅格化。

## 渲染方法的发展 {id="renderingMethod"}

### 图块渲染

在计算机图形起步早期，还没有各种图形库和接口供开发者使用。那时候通常是通过准备特殊的图块字符映射表，然后将场景通过对应字符打印在屏幕上，图形也以 2D 为主。

> 如果你曾经玩过 Famicom（红白机）等卡带游戏机，并且尝试过在游戏途中热拔出卡带，那么你一定见过这种场景：
> 
> ![热插拔游戏卡带](famicom_glitch.png "热插拔游戏卡带"){border-effect="rounded" width="300"}
> 
> 这种情况就是字符数据出错导致的图块映射错误。

### 固定管线渲染

随着计算机和 GPU 发展，3D 图形兴起，各种 3D 图形库也开始发展起来，OpenGL 就是其中之一。最早的图形库使用**固定渲染管线**，整个管线按照图形库的内置次序计算顶点光照、阴影等效果。开发者只可以配置渲染参数，没法精细控制每个几何体的效果，也没法自定义如何处理每个像素。

由于固定管线过于死板，希望能更自由地控制每个图元和像素的呼声越来越高，**可编程管线**应运而生。

> 如果将固定管线比作客观题（选择、填空），那么可编程管线就是主观题（简答、作文）。

### 向前渲染 {id="向前渲染法"}

在可编程管线刚刚发展起来的早期图形程序中，渲染思想是将一类几何体全部准备好并传入特定着色器，然后立即在传入的几何体上利用其几何信息计算诸如阴影和反射等效果，再输出到屏幕上。

因其将一批几何体一步到位绘制完成再绘制下一批，因而得名**向前渲染法**（Forward Shading）。这在 3D 图形程序刚起步的早期是没什么问题的，那时候场景中的几何体还不多，光照算法也不复杂。

然而随着场景几何体增多、几何体之间相对于视角的遮挡越来越频繁，光照算法也日渐复杂，这种做法开始产生越来越多不必要的开销。因为每个片段着色器都会将所有传入图元覆盖的所有像素计算一遍，即使像素内容会被更靠前的图元遮挡。

> 在现代 GPU 中，有一种名为**提前深度测试**（Early Depth Testing）的功能，可以在 GPU 上预先判断场景的几何遮挡情况，在着色器开始处理之前先丢弃将会或已经被遮挡的像素。

### 延迟渲染 {id="延迟渲染法"}

为了避免大量最终不可见的顶点在传入时就进行光照处理带来如此大的不必要开销，**延迟渲染法**（Deferred Shading）应运而生，它的思想是不再在传入几何体的阶段立即计算大多数效果，而是分为两个阶段：

1. **几何缓冲阶段**：通过着色器将纯色场景和诸如法线纹理和反射纹理等**全部作为纹理**映射到几何体上，再分别写入多个 [缓冲区](terms.md#缓冲区){summary=""} ，并通过 [颜色附件](terms.md#颜色附件){summary=""} 在着色器之间进行传递；
2. **延迟处理阶段**：之后的着色器读取对应缓冲区的这些信息，在**铺屏四边形**上统一计算光照、反射等其他效果。

延迟渲染还提供了一个优势，在向前渲染法中，像素着色器仅会对图元覆盖区域进行着色，这就导致了溢出类或者与其他类几何体有交互的特效难以正确绘制（比如镜头特效、模糊和 [环境光遮蔽](terms.md#ao){summary=""} 等），而延迟渲染法由于使用了铺屏四边形，所以可以在屏幕上任意位置进行绘制。

但同时由于其使用缓冲区的特性和场景几何信息的不可插值性 <sup>**1**</sup> ，[MSAA](shaderTech.md#msaa){summary=""} 之类提升内部分辨率再进行降采样的抗锯齿方法也就不能使用了 <sup>**2**</sup> 。

**[1]** 想象两个一远一近的像素，它们的颜色代表了它们的坐标，那么两个像素的颜色应该是**确定且等于**坐标的。如果将它们的颜色取平均后再赋回，那么它们的颜色就不等于坐标了，也就**失去了几何意义**。  
**[2]** 严格来说，如果在延迟渲染阶段不使用场景几何信息，或者保留降采样之前的几何信息，那么延迟渲染和 MSAA 也是可以共存的。

> 延迟渲染也不完全优于向前渲染，一些特殊材料只有在向前渲染下才能表现出应有的质地；由于延迟渲染使用多个屏幕尺寸的缓冲区，对显存的带宽要求较高，在移动设备上其性能也可能不如向前渲染；而且在半透明处理上延迟渲染也会更加复杂，因为缓冲区通常只能保存一层几何信息，需要创作者在处理时进行斟酌（大多数时候我们会选择保留固体信息，将半透明独立处理），甚至在渲染半透明时混用向前渲染。
> 
> 现代游戏通常会根据实际需求选择渲染方法，像是同属任天堂旗下的 IP，《斯普拉遁》就使用了向前渲染管线（场景较小，还有很多很鱿型的材质），而《塞尔达传说：旷野之息》则是延迟渲染管线（复杂的开放世界，并且对材质和半透明不敏感）。

<seealso>
  <category ref="related">
    <a href="terms.md#渲染模组和引擎" summary="总结了大多常用的渲染模组和引擎">术语表 - 渲染模组和引擎</a>
    <a href="shaderHistoryBE.md" summary=""/>
    <a href="shaderHistoryJE.md" summary=""/>
  </category>
  <category ref="advance">
    <a href="shaderTech.md" summary="对着色器的具体技术的科普"/>
  </category>
</seealso>
